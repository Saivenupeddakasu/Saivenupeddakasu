import numpy as np
import matplotlib.pyplot as plt

# 1. Define the function (loss function)
def f(x):
    return x**2

# 2. Define its derivative (gradient)
def df(x):
    return 2 * x

# 3. Set hyperparameters
learning_rate = 1
initial_x = 5.0
num_iterations = 50

# 4. Store values for plotting (optional but recommended)
x_values = [initial_x]
f_values = [f(initial_x)]

# 5. Gradient Descent loop
current_x = initial_x
for i in range(num_iterations):
    gradient = df(current_x)
    current_x = current_x - learning_rate * gradient

    x_values.append(current_x)
    f_values.append(f(current_x))

    # Optional: Print progress
    if i % 10 == 0:
      print(f"Iteration {i}: x = {current_x:.4f}, f(x) = {f(current_x):.4f}")

print(f"\nFinal x after {num_iterations} iterations: {current_x:.4f}")
print(f"Minimum f(x) found: {f(current_x):.4f}")

# 6. Visualization
x_plot = np.linspace(-6, 6, 400)
y_plot = f(x_plot)

plt.figure(figsize=(10, 6))
plt.plot(x_plot, y_plot, label='$f(x) = x^2$')
plt.scatter(x_values, f_values, color='red', s=50, zorder=5, label='GD Path')
plt.plot(x_values, f_values, color='red', linestyle='--', alpha=0.6) # Connect the dots
plt.title('Gradient Descent on $f(x) = x^2$')
plt.xlabel('$x$')
plt.ylabel('$f(x)$')
plt.grid(True)
plt.legend()
plt.show()
